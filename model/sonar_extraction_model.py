# -*- coding: utf-8 -*-
"""Sonar_extraction_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c7AT6MYHXK5TlwzNTQLzOK-jBF-tl6sE

#Installing necessary packages
"""

!pip install spacy transformers pdfplumber pandas sentence-transformers fuzzywuzzy python-Levenshtein
!python -m spacy download en_core_web_sm

import spacy
import pdfplumber
import pandas as pd

def read_cv_text(file_path):
    """Read and clean text from PDF CV."""
    try:
        with pdfplumber.open(file_path) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        text = text.replace("\n", " ").replace('\x00', '').strip()  # Clean newlines/spaces and null characters
        return text
    except Exception as e:
        print(f"Error: {e}")
        return ""

cv_text = read_cv_text("/content/amazon-software-engineer-resume-example.pdf")
print(cv_text[:300])
print("Length:", len(cv_text))

# #Function to extract explicit skills
# from huggingface_hub import snapshot_download
# import spacy

# model_path = snapshot_download("amjad-awad/skill-extractor")
# detect = spacy.load(model_path)
# if "sentencizer" not in detect.pipe_names:
#     detect.add_pipe("sentencizer")

# def extract_explicit_skills(text):
#     """Extract explicit skills using custom spaCy model with deduplication."""
#     skill = detect(text)
#     skills = []
#     seen = set()  # Set to track unique skills
#     for ent in skill.ents:
#         if ent.label_ == "SKILLS":
#             name = ent.text.strip()
#             lower_name = name.lower() # For case-insensitive deduplication
#             if lower_name in seen:
#                 continue # Skip if skill already added

#             seen.add(lower_name)
#             sent_text = ent.sent.text if ent.sent else ent.text  # Fallback to entity itself
#             skills.append({
#                 "name": name,
#                 "confidence": 0.85,  # Estimate; boost later
#                 "evidence": [f"CV - {sent_text[:50]}..."]  # Snippet for trail
#             })
#     return skills

# explicit = extract_explicit_skills(cv_text);
# print(explicit)

!pip install torch

!pip install transformers
from transformers import pipeline

from transformers import pipeline, AutoTokenizer
import numpy as np
import re

# --- Global Model Initialization for Explicit Skills ---
# This will be loaded once when the application starts
know_model = "jjzha/jobbert_knowledge_extraction"
ner_know = pipeline("ner", model=know_model, aggregation_strategy="simple")
tokenizer_explicit = AutoTokenizer.from_pretrained(know_model)


def extract_explicit_skills(text, conf_threshold=0.95, source="CV"):
    """Extract hard (tech) skills with improved merging, cleaning, splitting, and dedup.
    Uses globally initialized ner_know and tokenizer_explicit.
    """
    text = re.sub(r'[\uf000-\uf8ff\x00]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()

    # Manual Token-based Chunking
    tokens = tokenizer_explicit.tokenize(text)
    all_chunks_text = []
    chunk_size_tokens = 400  # Default chunk size for processing
    overlap_tokens = 50      # Default overlap for processing
    current_start = 0
    while current_start < len(tokens):
        end_index = min(current_start + chunk_size_tokens, len(tokens))
        chunk_tokens = tokens[current_start:end_index]
        chunk_text = tokenizer_explicit.convert_tokens_to_string(chunk_tokens)
        all_chunks_text.append(chunk_text)

        if end_index == len(tokens):
            break
        current_start += (chunk_size_tokens - overlap_tokens)
        if current_start < 0: current_start = 0

    # Improved clean/detokenize
    def clean_detokenize(parts):
        # Merge subwords
        full = []
        for p in parts:
            if p.startswith('##') and full:
                full[-1] += p[2:]
            else:
                full.append(p)
        name = ' '.join(full).strip()
        # Strip junk: punctuation, dashes, extras
        name = re.sub(r'[\-;:]+', ' ', name).strip()
        name = re.sub(r'\s+', ' ', name)
        return name

    # Process model
    def process_model(ner_pipe, chunks_list, model_type):
        all_spans = []
        for i, chunk in enumerate(chunks_list):
            try:
                raw = ner_pipe(chunk)
                for res in raw:
                    res['chunk_id'] = i

                current_parts = []
                current_scores = []
                current_b_res = None
                for res in raw:
                    if res['entity_group'] == 'B':
                        if current_parts:
                            full_word = clean_detokenize(current_parts)
                            avg_score = np.mean(current_scores)
                            # Quality filter: skip junk/short
                            if avg_score >= conf_threshold and not full_word.startswith(('#', ';', ':')) and len(full_word) > 2:
                                all_spans.append({
                                    'word': full_word,
                                    'confidence': float(avg_score),
                                    'chunk_id': i,
                                    'b_res': current_b_res,
                                    'type': model_type
                                })
                        current_parts = [res['word']]
                        current_scores = [res['score']]
                        current_b_res = res
                    elif res['entity_group'] == 'I' and current_parts:
                        current_parts.append(res['word'])
                        current_scores.append(res['score'])
                # Last span
                if current_parts:
                    full_word = clean_detokenize(current_parts)
                    avg_score = np.mean(current_scores)
                    if avg_score >= conf_threshold and not full_word.startswith(('#', ';', ':')) and len(full_word) > 2:
                        all_spans.append({
                            'word': full_word,
                            'confidence': float(avg_score),
                            'chunk_id': i,
                            'b_res': current_b_res,
                            'type': model_type
                        })
            except Exception as e:
                print(f"Warning: Chunk {i} error ({model_type}): {e}")
        return all_spans

    # Run for hard skills
    hard_spans = process_model(ner_know, all_chunks_text, "hard_skill")

    # Dedup + evidence + split combined
    def build_skill_list(spans):
        skill_dict = {}

        def add_skill(name, conf, chunk_id, b_res, type_):
            norm = re.sub(r'[^a-z0-9 ]', '', name.lower()).strip()
            if norm not in skill_dict:
                skill_dict[norm] = {
                    "name": name,
                    "confidence": conf,
                    "category": type_,
                    "evidence": [],
                    "chunk_ids": [chunk_id]
                }
            else:
                old_conf = skill_dict[norm]["confidence"]
                skill_dict[norm]["confidence"] = np.mean([old_conf, conf])
                if chunk_id not in skill_dict[norm]["chunk_ids"]:
                    skill_dict[norm]["chunk_ids"].append(chunk_id)

            # Evidence (1 unique, cleaned)
            if b_res and not skill_dict[norm]["evidence"]:
                res = b_res
                # Get original chunk text for evidence
                chunk_text_original = all_chunks_text[res['chunk_id']]
                chunk_text_original = re.sub(r'[\uf000-\uf8ff\x00#]+', ' ', chunk_text_original)  # Clean evidence

                # Find approximate start of sentence in the original chunk text
                word_start = res['start']
                # Look for a preceding full stop to get context, or just take previous characters
                context_start = max(0, chunk_text_original.rfind('.', 0, word_start) + 1)
                # Extract a snippet around the skill
                snip = chunk_text_original[context_start : word_start + len(name) + 50].strip()
                evidence = f"{source} - {snip[:100]}..."
                skill_dict[norm]["evidence"].append(evidence)

        for span in spans:
            name = span['word']
            # Split if combined by comma
            if ',' in name:
                sub_skills = [s.strip() for s in name.split(',') if s.strip()]
                for sub in sub_skills:
                    add_skill(sub, span['confidence'], span['chunk_id'], span['b_res'], span['type'])
            else:
                add_skill(name, span['confidence'], span['chunk_id'], span['b_res'], span['type'])

        skills = list(skill_dict.values())
        return sorted(skills, key=lambda x: x["confidence"], reverse=True)

    hard_skills = build_skill_list(hard_spans)

    return hard_skills

result = extract_explicit_skills(cv_text, conf_threshold=0.85)
print(result)

"""#Implicit skills with Semantic Analysis"""

import pandas as pd
from sentence_transformers import SentenceTransformer, util

esco_df = pd.read_csv("skills.csv")
model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight embedder

# Pre-embed ESCO skills (run once)
esco_embeddings = model.encode(esco_df['label_cleaned'].tolist(), convert_to_tensor=True)

#Function for implicit skills
def extract_implicit_skills(text, threshold=0.55, source="CV"):
    """Infer implicit skills via semantic similarity. Uses globally initialized model and esco_embeddings."""
    sentences = text.split(". ")  # Simple split
    sent_embeddings = model.encode(sentences, convert_to_tensor=True)
    implicit_skills = []
    for i, sent_emb in enumerate(sent_embeddings):
        similarities = util.cos_sim(sent_emb, esco_embeddings)[0]
        max_sim_idx = similarities.argmax() # This returns a tensor containing the index
        max_sim = similarities[max_sim_idx].item()
        if max_sim > threshold:
            # Convert max_sim_idx to a Python integer before using with .iloc
            skill_name = esco_df['label_cleaned'].iloc[max_sim_idx.item()]
            implicit_skills.append({
                "name": skill_name,
                "confidence": max_sim,  # Similarity as confidence
                "evidence": [f"{source} - implicit from: {sentences[i][:50]}..."]
            })
    return implicit_skills

result = extract_implicit_skills(cv_text)
print(result)

def extract_skills(text):
    explicit = extract_explicit_skills(text)
    implicit = extract_implicit_skills(text)
    return explicit + implicit

extract = extract_skills(cv_text)
print(extract)

"""#Map extracted skills to ESCO Framework"""

from fuzzywuzzy import fuzz

def map_to_esco(skills, esco_path="skills.csv", match_threshold=85):
    """Fuzzy map to ESCO."""
    esco_df = pd.read_csv(esco_path)
    mapped_skills = []
    for skill in skills:
        best_match = None
        best_score = 0
        for esco_skill in esco_df['label_cleaned']:
            score = fuzz.token_sort_ratio(skill['name'].lower(), esco_skill.lower())
            if score > best_score:
                best_score = score
                best_match = esco_skill
        if best_score > match_threshold:
            skill['name'] = best_match  # Normalize to ESCO
            skill['framework'] = "ESCO"
            skill['confidence'] = min(1.0, skill['confidence'] + (best_score / 100) * 0.1)  # Slight boost
        mapped_skills.append(skill)
    return mapped_skills

mapped = map_to_esco(extract_skills(cv_text));
assert len(mapped) > 0, "No skills!";
print(mapped)

"""##Aggregation Implementation"""

!pip install requests beautifulsoup4

#Implementing source for GitHub (via API)

import requests
# Removed time.sleep as it was causing significant delays for production use cases

def fetch_github_text(url, max_repos=30):
    """
    Ultra-fast GitHub skill extraction using:
      • Repo name + description
      • Programming language
      • GitHub topics (e.g., 'machine-learning', 'flask')

    No READMEs -> no size/time issues.
    """
    try:
        username = url.rstrip("/").split("/")[-1]
        api_url = f"https://api.github.com/users/{username}/repos"
        headers = {'User-Agent': 'Mozilla/5.0'} # Keep this for basic identification

        # --- Fetch all public repos (paginated) ---
        repos = []
        page = 1
        while len(repos) < max_repos:
            resp = requests.get(
                api_url,
                params={'per_page': 100, 'page': page},
                headers=headers
            )
            if resp.status_code == 403: # Rate limit exceeded
                print("GitHub API rate limit exceeded. Please try again later or use an authenticated token.")
                break
            if resp.status_code != 200:
                print(f"GitHub API error: {resp.status_code} - {resp.text}")
                break
            batch = resp.json()
            if not batch:
                break
            repos.extend(batch)
            page += 1

        sentences = []

        for repo in repos[:max_repos]:
            name = repo["name"].replace("-", " ").replace("_", " ")
            desc = repo.get("description") or ""
            lang = repo.get("language") or ""
            topics = repo.get("topics", [])

            # --- Repo + description ---
            if desc:
                sentences.append(f"Developed {name} using {lang}: {desc}")
            else:
                sentences.append(f"Worked on a project called {name} in {lang}.")
            # --- Language as skill (explicitly added here for easier extraction later) ---
            if lang:
                sentences.append(f"Language: {lang}")

            # --- Topics as skills (cleaned) ---
            for topic in topics:
                clean_topic = topic.replace("-", " ").replace("_", " ").strip()
                if clean_topic:
                    sentences.append(f"Used {clean_topic} technology.")

        return " ".join(sentences).strip()

    except Exception as e:
        print(f"GitHub fetch error: {e}")
        return ""

# Step 2: Multi-Source Extraction

def extract_from_sources(cv_path, github_url):
    """Extract and map skills from all sources."""
    # CV
    cv_text = read_cv_text(cv_path)
    cv_skills = extract_skills(cv_text)
    for s in cv_skills:
        s['source'] = "CV"  # Add source tag
        s['evidence'] = [e.replace("CV - ", "CV - ") for e in s['evidence']]

    # GitHub
    gh_text = fetch_github_text(github_url)
    gh_skills = extract_skills(gh_text) if gh_text else []
    for s in gh_skills:
        s['source'] = "GitHub"
        s['evidence'] = [e.replace("CV - ", "GitHub - ") for e in s['evidence']]

    # Map all to ESCO
    all_skills = map_to_esco(cv_skills + gh_skills)
    return all_skills

# Test
cv_path = cv_text
gh_url = "https://github.com/karpathy"

all_skills = extract_from_sources(cv_path, gh_url)
print("Extracted Skills Sample:", all_skills)

def reduce_hallucinations(aggregated, min_conf=0.7, min_evidence=1):
    """Extra filter: Conf + evidence count."""
    return [s for s in aggregated if s['confidence'] > min_conf and len(s['evidence']) >= min_evidence]

def aggregate_skills(skills_list):
    """Aggregate: Dedup by name (lower), average/boost conf, combine evidence/sources."""
    skill_dict = {}
    for skill in skills_list:
        norm_name = skill['name'].lower()
        if norm_name not in skill_dict:
            skill_dict[norm_name] = skill.copy()
            skill_dict[norm_name]['sources'] = [skill['source']]
            skill_dict[norm_name]['evidence'] = skill['evidence']
        else:
            # Average conf + boost if multi-source
            old_conf = skill_dict[norm_name]['confidence']
            new_conf = (old_conf + skill['confidence']) / 2
            if skill['source'] not in skill_dict[norm_name]['sources']:
                new_conf += 0.05  # Small boost for cross-source validation
            skill_dict[norm_name]['confidence'] = min(1.0, new_conf)
            # Combine evidence/sources
            skill_dict[norm_name]['sources'].append(skill['source'])
            skill_dict[norm_name]['evidence'].extend(skill['evidence'])

    # Convert back to list, sort by conf descending
    aggregated = sorted(skill_dict.values(), key=lambda x: x['confidence'], reverse=True)
    return aggregated

# Update generate_profile to use aggregation
def generate_profile(cv_path, github_url=""):
    all_skills = extract_from_sources(cv_path, github_url)
    aggregated = aggregate_skills(all_skills)
    # Filter hallucinations
    aggregated = reduce_hallucinations(aggregated, min_conf=0.6, min_evidence=1)
    aggregated = [s for s in aggregated if s['confidence'] > 0.5]
    profile = {"skills": aggregated}
    return json.dumps(profile, indent=4)

# Test
profile_json = generate_profile(cv_path, gh_url)
print(profile_json)

"""##JOB DESCRIPTION PARSER"""

def parse_job_description(jd_text):
    """Extract/map JD skills (same as user)"""
    jd_skills = extract_skills(jd_text)
    return map_to_esco(jd_skills)

jd_text = "Job: Data Engineer. Requirements: Python, SQL, AWS experience. Lead teams."
jd_skills = parse_job_description(jd_text)
print(jd_skills)

def match_skills(user_skills, jd_skills):
    """Compare sets, score (match %), gaps."""
    user_set = {s['name'].lower() for s in user_skills}
    jd_set = {s['name'].lower() for s in jd_skills}
    matches = user_set.intersection(jd_set)
    score = (len(matches) / len(jd_set)) * 100 if jd_set else 0
    gaps = list(jd_set - user_set)
    return {
        "match_score": round(score, 2),
        "matches": list(matches),
        "gaps": gaps
    }

# Test with aggregated user_skills
result = match_skills(all_skills, jd_skills)  # From Phase 3
print(json.dumps(result, indent=4))